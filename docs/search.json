[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ディープラーニングによるカエル類の種の識別方法",
    "section": "",
    "text": "1 本ページは準備中です",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>本ページは準備中です</span>"
    ]
  },
  {
    "objectID": "index.html#sec-index-contents",
    "href": "index.html#sec-index-contents",
    "title": "ディープラーニングによるカエル類の種の識別方法",
    "section": "2.1 内容",
    "text": "2.1 内容\n立ち位置としては論文の補遺ですが、本ページ単体でも内容が理解できるように配慮しています。論文本体から分離させることで、具体的なソフトの扱いやコードを載せられるようにというのと、手法の発展が早い分野であるため、よりよいソフトが出てきたらその内容を加えてアップデートしやすいようにという意図があります。\nディープラーニングを使って音響モニタリングデータを解析するには、訓練データの用意、モデルの訓練・評価を経て、データ解析を行います。これらの内容を後のページで扱います。\n\nChapter 2 録音した野外データからアノテーションを行う\nChapter 3 そのアノテーションファイルをBirdNETで扱いやすいよう3秒毎に分割\nChapter 4 BirdNETを用いたモデルの訓練\nChapter 5 新規データの解析\nChapter 6 コマンドラインでの利用\nChapter 7 モデルの性能評価\n\n一般的に、ディープラーニングを扱う際は、PyTorchやTensorflowといった専用のPythonライブラリを用いることが多いです。しかしそれぞれのライブラリの扱いに習熟するのはやはり大変です。最近ではBirdNETという生物音響に特化したソフトを使うことで、とくにWindowsではコードを書かずにモデルの訓練が可能ですので、ここではBirdNETを使った訓練方法を紹介します。\nモデルの訓練でコードを書く必要が減ったとはいえ、データの中間処理や解析結果をまとめるにあたっては一定のコードを書く必要があります。ここでは生態学分野で広く使われているR言語を利用しています。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>本ページは準備中です</span>"
    ]
  },
  {
    "objectID": "index.html#sec-index-dataset",
    "href": "index.html#sec-index-dataset",
    "title": "ディープラーニングによるカエル類の種の識別方法",
    "section": "2.2 データセット",
    "text": "2.2 データセット\nモデルの訓練に用いるサンプルデータセットおよびコードはこちらから入手できます。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>本ページは準備中です</span>"
    ]
  },
  {
    "objectID": "index.html#sec-index-reference",
    "href": "index.html#sec-index-reference",
    "title": "ディープラーニングによるカエル類の種の識別方法",
    "section": "1.3 引用方法",
    "text": "1.3 引用方法\nこの内容がもし役立てば以下の文献を引用ください。\n木村楓. 2024. ディープラーニングによるカエル類の音響モニタリング入門. 爬虫両棲類学会報 2024(2):XXX-XXX.\n内容について間違いや改善点を見つけたら、issueにてご教示いただけますと大変ありがたいです。\n\n\n\n\n木村楓. 2024. “ディープラーニングによるカエル類の音響モニタリング入門.” 爬虫両棲類学会報 2024 (2).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>はじめに</span>"
    ]
  },
  {
    "objectID": "annotation.html",
    "href": "annotation.html",
    "title": "2  アノテーション",
    "section": "",
    "text": "2.1 はじめに\nここでは、野外に設置した音声レコーダーを用いてカエルの鳴き声を録音した、あるいは録音する予定であるという状況を想定し、得られた録音データの解析方法を扱います。同様の手法がカエル以外の生物でも適用可能ですが、コウモリなど可聴域を超える音声については後に紹介するBirdNETが未対応です。\n種を判定するためのディープラーニングモデルを作るには、まず対象種の訓練データが必要となります。このために、データに対し種名等のラベルを付けることをアノテーションといいます。日本産のカエル類ではアノテーション済みの公開データは非常に少ないため、多くの場合、調査者がアノテーションを行うか、あるいは合成音声を作成する必要があります。ここでは野外環境に近いデータに基づき訓練を行うため、野外録音をアノテーションする方法を紹介します。\nアノテーション用のソフトとして、生物音響分野ではRaven Pro/Lite（Proは有料版。無料版のLiteでもアノテーションは可能）やAudacityを利用している例が多いようです。これらのソフトは音響解析一般を扱うもので、必ずしも大量のファイルをアノテーションすることに最適化されているわけではありませんが、ユーザーが多く動作が安定しているため、扱いやすいと思いました。ここではRaven Liteを用いてアノテーションする方法を紹介します。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>アノテーション</span>"
    ]
  },
  {
    "objectID": "annotation.html#sec-annotation-setup",
    "href": "annotation.html#sec-annotation-setup",
    "title": "2  アノテーション",
    "section": "2.2 セットアップ",
    "text": "2.2 セットアップ\nインストール\nRaven Liteのページからソフトのインストールを行ってください。インストール方法については全国鳥類繁殖分布調査による解説 https://www.bird-atlas.jp/data/download.pdf があります。\nインストールが完了したらRaven Liteを起動します。以下では初期設定とアノテーションの流れを扱います。より詳しいRavenの操作方法はYoutubeでわかりやすい 解説 があります。動画は英語ですが必要に応じて字幕&gt;自動翻訳を使うこともできます。\n設定\n続いてRavenの表示設定をアノテーションしやすいように調整します。試しにサンプルデータ（Section 1.2）でダウンロードしたデータのうちの一つ（sample_data/train/20220315_090000.WAV）をRavenで開いてみましょう。ファイルをRavenの画面にドラッグアンドドロップし、OKを押すと Figure 2.1 のような表示になるかと思います（Macでの画面です）。\n\n\n\n\n\n\nサンプルデータについて\n\n\n\n訓練用のサンプルデータはsample_dataフォルダ内のtrainフォルダに入っています。このようなフォルダの構造をsample_data/trainと記しています。\nこのデータは京都市の山中の小さな池にAudiomoth（OpenAcoustics社製）というレコーダーを設置して録音されたものです。各ファイルの長さは60秒です。もともと Kimura and Sota (2023) で扱ったデータをRavenを用いてアノテーションし直したものになります。すべてのサンプル音声データはアノテーション済みで、サンプルの音声データと同じフォルダ内に、アノテーション結果を保存したtxtファイルも含めています。\n\n\n\n\n\n\n\n\nFigure 2.1: Ravenの初期表示\n\n\n\n画面上部には波形が、下部はスペクトログラムが表示されています。この音源にはヒキガエルの鳴き声が含まれていますが、デフォルトの設定では表示が小さくアノテーションしにくいので、設定を変更します。定まった方法があるわけではなく好みの問題ですが、個人的には以下のような設定で表示させています。\n\n波形を非表示にする（Waveform 1 のチェックを外す Figure 2.2)\n4段表示にする（Lines: 1 を Lines: 4に変更する）。\n白黒表示にして、明るさとコントラストを少し上げる（初期設定の50から、70くらいに変更）。\n表示される周波数の上限を7 kHzほどまで下げる（画面右端の＋マークをクリックして、周波数方向に拡大表示をする）。\n\n\n\n\n\n\n\nFigure 2.2: Ravenの表示設定変更。説明の文字が見えやすいよう、画面を白黒画像に変換しています\n\n\n\nさらに、\n\nアノテーションしたとき種名ラベルが画面上に表示されるようにする。\n\n画面上で右クリック &gt; Configure Selection Labels &gt; Available ItemsからAnnotationを選択後、「&lt;&lt;」 ボタンを押し、Displayed Itemsに移動させる。同時にFont sizeを14まで小さくする。\n\n\nこれにより Figure 2.3 のように表示されます。\n\n\n\n\n\n\nFigure 2.3: 表示設定を調整後\n\n\n\nこの状態で画面を右クリックし、Window Presets &gt; Save as… から表示設定を保存してください。たとえばFrogAnnotationといった後からわかる名前にします。次回以降、音声を読み込む際にはWindow PresetをFrogAnnotationに変更することで 、ここで定めた表示設定で音声を読み込むことができます（Figure 2.4）。\n\n\n\n\n\n\nFigure 2.4: 音声読み込みの際にWindow Presetを新しく保存したプリセットに変更する",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>アノテーション</span>"
    ]
  },
  {
    "objectID": "annotation.html#sec-annnotation-annotation",
    "href": "annotation.html#sec-annnotation-annotation",
    "title": "2  アノテーション",
    "section": "2.3 アノテーション",
    "text": "2.3 アノテーション\n\n2.3.1 鳴き声のアノテーション方法\n\n再生ボタン（▶）から音声を聞いて、何の種がいつ鳴いているかを確認します\n鳴き声の範囲をマウスでドラッグすると、四角いボックスが赤色で描かれます\nそのままEnterをクリックするとAnnotation（どんなラベルを付けるか）を聞かれるので、種名を記録します（Figure 2.5）。\n\n私は Cañas et al. (2023) をまねて、学名の属名と種小名の冒頭３文字をAnnotationに記載しています。たとえばニホンヒキガエル Bufo japonicus であればBUFJAPです。このとき「Use specified value as default」にチェックをいれると、同じファイルのなかで別の領域を選択した場合にBUFJAPが記入された状態から始まるので、何度も同じ文字を打ち込む必要がなく、ミスを減らせます。\n\n\n\n\n\n\nFigure 2.5: アノテーションの例\n\n\n\nこれをすべての鳴き声について続けると、 Figure 2.6 のようアノテーションできます。今回はすでにアノテーションされたデータをdataフォルダに含めているので保存する必要はありませんが、実際に自分のデータをアノテーションする場合は File &gt; Save Selection Table “Table 1” から結果を保存できます。\n\n\n\n\n\n\n\n\n2022年3月15日の音声\n\n\n\n\n\n\n\n2022年3月26日の音声\n\n\n\n\n\n\nFigure 2.6: アノテーションを完了したファイルの例\n\n\n\n\n\n2.3.2 アノテーションの基準\nどのようにアノテーションするかについて定まった方法はありません。北米の鳥(Chronister et al. (2021)) やブラジルのカエル(Cañas et al. (2023))についてアノテーションされたデータセットが公開されているため、手法の検討のうえで参考にしました。\nなお、サンプルデータの作成にあたっては以下の基準でカエルの鳴き声をアノテーションしました。\n\n同種の鳴き声で、鳴き声の間隔が0.5秒未満のものは一つの鳴き声として四角で囲った。\n音を聞くとかろうじて聞こえるものでも、スペクトログラム上でほとんど表示されない鳴き声については無視した。\n種不名の鳴き声は「frog」というラベルをつけた。\n\n\n\n2.3.3 どれだけアノテーションするのか\nアノテーションは時間と手間のかかる作業です。どれだけアノテーションする必要があるか、一概には言えないようですが、たとえばBirdNETのドキュメントでは各種最低100サンプル以上を推奨しています。BirdNETの場合、3秒間を一つの解析処理の単位として扱うため、3秒の音声ファイルが100個（合計300秒）ということになります。個人的な経験でも、背景音のノイズが比較的少なく、鳴き声が明瞭に聞こえる場合にはこれくらいの分量で一定の精度が出るような感覚があります。各種100サンプル以上用意できたら一度モデルを訓練してみて、目標とする性能に達していなければ、アノテーションを追加するという作業を繰り返すのが良いように思います。\nディープラーニングでは訓練データに含まれないような音声があると適切に判断できないことが多いので、なるべく様々な種類の背景音（雨・救急車の音・異なる鳥や虫の声など）が含まれるようアノテーションする音声ファイルを選ぶことが推奨されます。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>アノテーション</span>"
    ]
  },
  {
    "objectID": "annotation.html#sec-annotation-others",
    "href": "annotation.html#sec-annotation-others",
    "title": "2  アノテーション",
    "section": "2.4 その他のアノテーションソフト",
    "text": "2.4 その他のアノテーションソフト\nはじめに（2.1） で述べたように、Raven Liteは大量のファイルをアノテーションすることに必ずしも最適化されていないと感じます。生物音響用のアノテーションに特化したソフトを開発している論文も複数あり、たとえば Deep Audio Segmenter (Steinfath et al. (2021)) や Whombat (Balvanera et al. (2023)) が挙げられ、それぞれに面白い機能を持っています。生物音響解析ソフトまとめているサイトには他の選択肢も載っています。\nこのうちのいくつかMacで使えるものを試してみましたが、インストールにPythonの環境構築が必要だったり、動作が不安定だったりと、自分で使う分には慣れればなんとかなるものの、他人に勧めるという観点からはRavenが現状一番かなと感じました。\n\n\n\n\nBalvanera, Santiago Martinez, Oisin Mac Aodha, Matthew J Weldy, Holly Pringle, Ella Browning, and Kate E Jones. 2023. “Whombat: An Open-Source Annotation Tool for Machine Learning Development in Bioacoustics.” arXiv. https://arxiv.org/abs/2308.12688.\n\n\nCañas, Juan Sebastián, María Paula Toro-Gómez, Larissa Sayuri Moreira Sugai, Hernán Darío Benítez Restrepo, Jorge Rudas, Breyner Posso Bautista, Luís Felipe Toledo, et al. 2023. “A Dataset for Benchmarking Neotropical Anuran Calls Identification in Passive Acoustic Monitoring.” Sci Data 10 (1): 771. https://doi.org/10.1038/s41597-023-02666-2.\n\n\nChronister, Lauren M, Tessa A Rhinehart, Aidan Place, and Justin Kitzes. 2021. “An Annotated Set of Audio Recordings of Eastern North American Birds Containing Frequency, Time, and Species Information.” Ecology 102 (6): e03329. https://doi.org/10.1002/ecy.3329.\n\n\nKimura, Kaede, and Teiji Sota. 2023. “Evaluation of Deep Learning-Based Monitoring of Frog Reproductive Phenology.” Ichthyology & Herpetology 111 (4): 563–70. https://doi.org/10.1643/h2023018.\n\n\nSteinfath, Elsa, Adrian Palacios-Muñoz, Julian R Rottschäfer, Deniz Yuezak, and Jan Clemens. 2021. “Fast and Accurate Annotation of Acoustic Signals with Deep Neural Networks.” eLife 10: e68837. https://doi.org/10.7554/eLife.68837.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>アノテーション</span>"
    ]
  },
  {
    "objectID": "data_prep.html",
    "href": "data_prep.html",
    "title": "3  訓練データの整理",
    "section": "",
    "text": "3.1 はじめに\n続いて、アノテーションされた音声を訓練データとして扱いやすい形に変換します。\n今回扱うBirdNETモデルの場合、３秒間の音声データを１サンプルとして訓練や解析を行う仕様です。３秒以上の音声ファイルは中央の３秒間が抽出され、３秒未満なら３秒間になるようノイズが付加されます（公式ドキュメント）。ディープラーニングのモデルでは、多くの場合、計算を高速化するため特定の時間間隔で音声データを区切って扱う仕様になっています。\nBirdNETが必要とする訓練データを用意するため、アノテーションした音声データに以下の処理を行います。\n以下、Rを用いてその作業を自動化するサンプルコードを提示します。このページの内容はサンプルデータ内にある、data_prep.qmdを開いて実行することができます。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>訓練データの整理</span>"
    ]
  },
  {
    "objectID": "data_prep.html#sec-prep-intro",
    "href": "data_prep.html#sec-prep-intro",
    "title": "3  訓練データの整理",
    "section": "",
    "text": "音声データを３秒ずつに区切る\nカエルの種ごとにフォルダを作成し、その中に３秒間の音声データを入れていく\n\n\nたとえば３秒間の音声にヒキガエルが含まれているならBUFJAPという名称のフォルダに入れ、シュレーゲルアオガエルが含まれているならZHASCHに入れます。\nどの種も含まれていない背景音ならbackgroundという名称のフォルダを作って、そこに入れます。\nヒキガエルとシュレーゲルアオガエルの両種が含まれる場合は、保存フォルダの名称を「BUFJAP,ZHASCH」のようにカンマ区切りで両種の名前をつけます（種名の間にはスペースを入れない）。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>訓練データの整理</span>"
    ]
  },
  {
    "objectID": "data_prep.html#sec-prep-useR",
    "href": "data_prep.html#sec-prep-useR",
    "title": "3  訓練データの整理",
    "section": "3.2 Rの利用",
    "text": "3.2 Rの利用\n以下の内容はRの知識を必要とします。Rは統計解析に重きをおいたプログラミング言語です。Rについて入門書が多数出ており、そちらを参照するのも良いですし、オンライン上でも私たちのRや、R for Data Science (2e)の丁寧な解説を読むことができます（後者の日本語版は「Rではじめるデータサイエンス 第2版」として出版）。\n生態学分野ではRを利用している人が多いためRでのコードを記しますが、Pythonなど好みの言語で同様の処理を行うことができます。\n\n\n\n\n\n\nNote\n\n\n\n今回紹介しているディープラーニングによる音響解析手順のなかで、このデータ整理の部分はあまりシンプルにできておらず、それなりに長いRのコードを解釈する必要があります。RavenとBirdNETがより統合されてくると、このあたりももっと簡単になるかもしれません。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>訓練データの整理</span>"
    ]
  },
  {
    "objectID": "data_prep.html#sec-prep-setup",
    "href": "data_prep.html#sec-prep-setup",
    "title": "3  訓練データの整理",
    "section": "3.3 セットアップ",
    "text": "3.3 セットアップ\n以下のパッケージを使用します。\n\n# 必要なライブラリの読み込み\nlibrary(tuneR)\nlibrary(tidyverse)\n\n\n\n\n\n\n\nNote\n\n\n\n以下に記載しているコードが適切に実行されるには、Rのワーキングディレクトリの直下にsample_dataフォルダがある必要があります。ダウンロードしたサンプルデータ・コード（Section 1.2）から、frog-DL-example.RprojをクリックしてRを開くと、ワーキングディレクトリが適切に設定されるはずです。\nなおワーキングディレクトリはコンソールにgetwd()と打ち込んで確認できます。\n\n\nまずサンプルデータに含まれているアノテーションを確認します。このために、list.files()関数によりsample_data/trainフォルダに入っているアノテーションファイルの一覧を取得し、その冒頭6個を表示させてみます。\n\nannotations &lt;- list.files(\"sample_data/train\", \"selections.txt\")\nprint(head(annotations))\n\n[1] \"20220315_090000.Table.1.selections.txt\"\n[2] \"20220315_100000.Table.1.selections.txt\"\n[3] \"20220315_110000.Table.1.selections.txt\"\n[4] \"20220315_130000.Table.1.selections.txt\"\n[5] \"20220316_090000.Table.1.selections.txt\"\n[6] \"20220316_120000.Table.1.selections.txt\"\n\n\n各ファイルは{録音日}_{録音時刻}.Table.1.selections.txt という命名規則になっています。 これらのアノテーションファイルに対応する音声もフォルダ内にあり、そのファイル名は録音日時を表す部分に拡張子.WAV がついた形をしています。\n\naudio &lt;- list.files(\"sample_data/train\", \"WAV\")\nprint(head(audio))\n\n[1] \"20220315_090000.WAV\" \"20220315_100000.WAV\" \"20220315_110000.WAV\"\n[4] \"20220315_130000.WAV\" \"20220316_090000.WAV\" \"20220316_120000.WAV\"\n\n\n最終的にこれらの音声ファイルを３秒間ずつに区切り、鳴いている種の名前がついたフォルダに保存していくのですが、そのために使ういくつかの関数を定義していきます。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>訓練データの整理</span>"
    ]
  },
  {
    "objectID": "data_prep.html#関数の定義",
    "href": "data_prep.html#関数の定義",
    "title": "3  訓練データの整理",
    "section": "3.4 関数の定義",
    "text": "3.4 関数の定義\n\n3.4.1 ラベル作成関数\nRavenで作成されたselections.txtファイルを読み込むにはread_delim()関数を用います。このファイルは表の各列がタブによって区切られた形式をしているため、delim = \\t と指定することで、適切に読み込むことができます。たとえば\n\ndf &lt;- read_delim(file.path(\"sample_data/train\", annotations[1]), delim = \"\\t\", show_col_types = FALSE)\nknitr::kable(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelection\nView\nChannel\nBegin Time (s)\nEnd Time (s)\nLow Freq (Hz)\nHigh Freq (Hz)\nDelta Time (s)\nDelta Freq (Hz)\nAvg Power Density (dB FS/Hz)\nAnnotation\n\n\n\n\n1\nSpectrogram 1\n1\n15.75682\n16.18630\n294.314\n1846.154\n0.4295\n1551.840\n-47.19\nBUFJAP\n\n\n3\nSpectrogram 1\n1\n18.03148\n19.11313\n347.826\n1712.375\n1.0817\n1364.549\n-58.90\nBUFJAP\n\n\n4\nSpectrogram 1\n1\n26.27742\n29.71326\n187.291\n1846.154\n3.4358\n1658.863\n-54.75\nBUFJAP\n\n\n5\nSpectrogram 1\n1\n32.06475\n34.95977\n321.070\n7036.789\n2.8950\n6715.719\n-56.51\nBUFJAP\n\n\n6\nSpectrogram 1\n1\n43.66724\n44.11263\n401.338\n6795.987\n0.4454\n6394.649\n-50.14\nBUFJAP\n\n\n11\nSpectrogram 1\n1\n47.96549\n48.45974\n394.010\n5459.849\n0.4943\n5065.839\n-48.19\nBUFJAP\n\n\n\n\n\nなお末尾の show_col_types は、列の形式（数値か文字列かなど）を表示するかどうかを決めるオプションです。また、knitr::kable()部分は、オンライン上で表が綺麗に表示されるためだけのもので、ここでは無視して問題ありません。\nつづいて３秒間ずつ（０から３秒目、３から６秒目、６から９秒目…のように）アノテーションされた種を抜き出すことを考えます。例えば前述の表で、15から18秒目までに含まれるアノテーションを抜き出すには、\n\nlabel_df &lt;- df |&gt; \n  filter(`Begin Time (s)` &lt; 18 & `End Time (s)` &gt; 15) |&gt; \n  distinct(Annotation)  # 同じ種のラベルがあった場合に重複を削除する\n\nknitr::kable(label_df)\n\n\n\n\nAnnotation\n\n\n\n\nBUFJAP\n\n\n\n\n\n15から18秒の間ではニホンヒキガエルの鳴き声が一つ含まれていましたが、何の種も鳴いていない場合、あるいは複数種が鳴いている場合もあります。これに対処するため、以下のようにラベルを取得します。\n\nlabel_df &lt;- df |&gt; \n  filter(`Begin Time (s)` &lt; 3 & `End Time (s)` &gt; 0) |&gt;   # ０から３秒目には何も鳴いていない\n  distinct(Annotation) |&gt; \n  arrange(Annotation)  # アノテーションをアルファベット順に並べ替え\n\nlabel &lt;- if(nrow(label_df) == 0) \"background\" else str_flatten(label_df$Annotation, collapse = \",\")\nprint(label)\n\n[1] \"background\"\n\n\nここでアノテーションが含まれない部分は、表の行数が0になることを利用し、if(nrow(label_df) == 0)で判定して、backgroundというラベル名をつけています。また複数種の鳴き声が含まれる場合は、str_flatten()関数によりカンマで連結されたラベル名とします（例：BUFJAP,ZHASCH）。\nここまでのことを利用して、アノテーションファイルを読み込み、3秒ごとのラベルを作成する関数を作成します。０から３秒目、３から６秒目…と逐次的な操作を行うため、for文を用いて繰り返しの処理を行っています。\n\ncreate_labels &lt;- function(annotation){\n  df &lt;- read_delim(annotation, delim = \"\\t\", show_col_types = FALSE) |&gt;   # アノテーションファイルを読み込む\n    filter(Annotation != \"frog\")  # 種不明を表すfrogを除く\n  \n  label_vec &lt;- vector(mode = \"character\")  # ラベルを格納するベクトルを初期化\n  \n  for(i in 1:20){\n    start &lt;- (i-1) * 3  # i番目の音声クリップの開始時間\n    end &lt;- i * 3  # i番目の音声クリップの終了時間\n    # 音声クリップ内に含まれるアノテーションを整理\n    label_df &lt;- df %&gt;%\n      # startからendと重複する部分のあるアノテーションのみを抜き出す\n      filter(`Begin Time (s)` &lt; end & `End Time (s)` &gt; start) %&gt;%  \n      distinct(Annotation) %&gt;%  # 同じ種のラベルの重複を削除\n      arrange(Annotation)  # アノテーションをアルファベット順に並べ替え\n    \n    # クリップ内にアノテーションがない場合は\"background\"、ある場合はアノテーションをカンマ区切りで連結\n    label &lt;- if(nrow(label_df) == 0) \"background\" else str_flatten(label_df$Annotation, collapse = \",\")\n    label_vec[i] &lt;- label  # ラベルをリストに格納\n  }\n  \n  return(label_vec)\n}\n\nこの関数をテストしてみます。\n\nlabels &lt;- create_labels(file.path(\"sample_data/train\", annotations[1]))\nprint(labels)\n\n [1] \"background\" \"background\" \"background\" \"background\" \"background\"\n [6] \"BUFJAP\"     \"BUFJAP\"     \"background\" \"BUFJAP\"     \"BUFJAP\"    \n[11] \"BUFJAP\"     \"BUFJAP\"     \"background\" \"background\" \"BUFJAP\"    \n[16] \"BUFJAP\"     \"BUFJAP\"     \"background\" \"background\" \"background\"\n\n\n60秒間を3秒毎に区切ったので、20個のラベルが作成されました。ニホンヒキガエルの鳴き声が含まれる音声クリップについてはBUFJAP、アノテーションがついていない音声クリップについてはbackgroundというラベルがついています。\n\n\n3.4.2 音声の分割\n続いて、音声ファイルを読み込み、３秒ごとの短い音声クリップに分割する関数を定義します。まずRで音声ファイルを扱うためにtuneRのreadWave()関数により音声データを読み込みます。\n\nwave &lt;- readWave(file.path(\"sample_data/train\", audio[1]))\n\nこの音声（audio[1] = 20220315_090000.WAV ）の概要を確認すると、\n\nprint(wave)\n\n\nWave Object\n    Number of Samples:      1920000\n    Duration (seconds):     60\n    Samplingrate (Hertz):   32000\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n60秒間（Durationに着目）のモノラル音源で、サンプリング周波数（Samplingrate）は32000 Hz であることがわかります。この60秒の音声のうち、０から３秒目までは以下のように取得できます。\n\ny &lt;- wave@left          # 音声データの具体的な数値を取得\nsr &lt;- wave@samp.rate    # サンプリング周波数を取得\n\nclip &lt;- y[1 : (3*sr)]   # ０から３秒目までを取得\n\nここで、音声データは毎秒サンプリング周波数個（この音源では32000個）の数値によって表現されているため、１から「3×サンプリング周波数」番目までの音声データを切り取ると冒頭３秒の音声を取得できることに注意してください。\n以上をふまえて、音声を3秒ずつにカットする関数を定義します。\n\ncut_audio &lt;- function(audio_file){\n  wave &lt;- readWave(audio_file)    # 音声ファイルの読み込み\n  y &lt;- wave@left                  # 波形データを取得\n  sr &lt;- wave@samp.rate            # サンプリング周波数を取得\n  \n  # 分割された音声を格納するリストを作成\n  audio_list &lt;- list()\n  \n  for(i in 1:20){\n    start &lt;- (i-1) * 3 * sr + 1   # 音声クリップの開始位置を計算\n    end &lt;- i * 3 * sr             # 音声クリップの終了位置を計算\n    clip &lt;- y[start:end]          # 3秒ごとに分割\n    audio_list[[i]] &lt;- Wave(left = clip, samp.rate = sr, bit = 16)  # 分割した音声をリストに格納\n  }\n    \n  return(audio_list)\n}\n\nこれも関数がどんな働きをするか、テストしてみます。\n\ntest &lt;- cut_audio(file.path(\"sample_data/train\", audio[1]))\nprint(test[1:3])\n\n[[1]]\n\nWave Object\n    Number of Samples:      96000\n    Duration (seconds):     3\n    Samplingrate (Hertz):   32000\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n[[2]]\n\nWave Object\n    Number of Samples:      96000\n    Duration (seconds):     3\n    Samplingrate (Hertz):   32000\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n[[3]]\n\nWave Object\n    Number of Samples:      96000\n    Duration (seconds):     3\n    Samplingrate (Hertz):   32000\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\nこのように、３秒ずつに区切られた音声データ（Durationに着目）が生成されています。\n\n\n3.4.3 音声とラベルの保存\n音声データの保存にはwriteWave()関数が利用できます。この関数は、第一引数として保存する音声データを、第二引数として保存する際のファイル名をとります。音声データは先程定義したcut_audio()関数により取得できますので、保存先のフォルダと、保存する際のファイル名を作成することを考えます。\nまず保存先のフォルダとしてdatasetという名称のフォルダを作り、その中にbackground,BUFJAP,ZHASCHといった種名のフォルダを含めることにします。\n\nsave_dir &lt;- file.path(\"dataset\", labels[1])\nsave_dir\n\n[1] \"dataset/background\"\n\n\nすでにdataset/backgroundのようなフォルダが存在している場合にはこれで問題なく音声を保存できますが、最初の一回はフォルダの作成から始める必要があります。Rでは`dir.create()によりフォルダを作成することが出来ます。\n\ndir.create(save_dir, recursive = TRUE, showWarnings = FALSE)  # フォルダが存在しない場合は作成\n\nこれにより、ワーキングディレクトリにdataset/backgroundという空のフォルダが作成されます。ラベルがbackgroundではなくBUFJAPやZHASCHであった場合にも、それぞれの種名がついたフォルダが自動で作成されます。\nまた、保存する際のファイル名（上流のパスを含む）は、以下のように設定できます。ここでは{録音日}_{録音時刻}_{番号}.WAVという命名規則でファイル名をつけています。\n\nfile.path(\n  \"dataset\",\n  labels[1],\n  str_replace(audio[1], \".WAV\", str_c(\"_\", \"1\", \".WAV\"))\n)\n\n[1] \"dataset/background/20220315_090000_1.WAV\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>訓練データの整理</span>"
    ]
  },
  {
    "objectID": "data_prep.html#処理の実行",
    "href": "data_prep.html#処理の実行",
    "title": "3  訓練データの整理",
    "section": "3.5 処理の実行",
    "text": "3.5 処理の実行\nここまで定義してきた関数を用いて、全てのアノテーションファイルに対して上記の処理を実行します。\n\n# アノテーションファイル一覧を取得\nannotations &lt;- list.files(\"sample_data/train\", pattern = \"selections.txt\")\n\n# 音声クリップへの分割と保存を実行\nfor(file in annotations){\n  labels &lt;- create_labels(file.path(\"sample_data/train\", file))          # ラベルを作成\n  audio_file &lt;- str_replace(file, \"Table.1.selections.txt\", \"WAV\")       # アノテーションに対応する音声ファイル名を取得\n  cutted_audio &lt;- cut_audio(file.path(\"sample_data/train\", audio_file))  # 音声ファイルを分割\n  \n  for(i in 1:20){\n    # フォルダが存在しない場合は作成\n    dir.create(file.path(\"dataset\", labels[i]), recursive = TRUE, showWarnings = FALSE)         \n    # 音声を保存\n    writeWave(object = cutted_audio[[i]],\n              filename = file.path(\"dataset\",\n                                   labels[i], \n                                   str_replace(audio_file, \".WAV\", str_c(\"_\", i, \".WAV\"))),  # ファイル名に番号(1,2,3...)をつけて保存\n              extensible = FALSE  # モノラルでも片耳からの音声にならないよう設定\n              )\n  }\n}\n\nうまくいっていれば、現在の作業ディレクトリにdatasetというフォルダが作成され、そのなかに「background」「BUFJAP」「BUFJAP,ZHASCH」「ZHASCH」があり、それぞれに対応する音声ファイルが保存されているという状況ができたはずです。\n次は、モデルの訓練に移ります。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>訓練データの整理</span>"
    ]
  },
  {
    "objectID": "train_gui.html",
    "href": "train_gui.html",
    "title": "4  モデルの訓練（GUI）",
    "section": "",
    "text": "4.1 はじめに\n訓練データを用意できたらBirdNET-Analyzer（以下BirdNET）というソフトを利用してモデルの訓練を行います。BirdNETは鳥類を主とした鳴き声によって訓練済みのモデルで、2024年7月現在、世界で6千種以上の生物の鳴き声を識別することができる大規模なものです(Kahl et al. (2021))。デフォルトのモデルで識別できる種のなかにはカエルも何種か含まれていますが、基本的に北米の種で、日本にも分布しているのは外来種であるウシガエルのみです。\nしかしBirdNETは訓練データさえこちらで用意すれば、デフォルトのモデルに含まれていない種でも認識するよう訓練できます。多様な鳴き声で事前に訓練されているため、音声に含まれる各種の特徴を抽出する能力が高く、比較的少数のサンプルでも高性能なモデルを作りやすいとされています(Ghani et al. (2023))。\nまたBirdNETはGUI（グラフィカルユーザーインターフェイス）を提供していることも特徴で、とくにWindowsを使っている場合はインストールからモデルの訓練・解析まで、コードを書かずに完了できるため、とっつきやすいソフトです。CLI（コマンドラインインターフェイス）も用意されているので、解析手順の再現性を高めたい場合にはCLIを利用することができます。\n公式にきちんと操作方法解説されているため、ここでの内容は、シンプルな訓練手順を日本語で簡単に紹介することにとどめます。詳細な訓練のオプションやその他の機能については、以下のドキュメントに書かれています。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>モデルの訓練（GUI）</span>"
    ]
  },
  {
    "objectID": "train_gui.html#sec-gui-intro",
    "href": "train_gui.html#sec-gui-intro",
    "title": "4  モデルの訓練（GUI）",
    "section": "",
    "text": "Note\n\n\n\nMacでもGUIが使えますが、インストールのために仮想環境を用意し、適切なバージョンのPythonとライブラリーを入れる必要があります。この手順は Chapter 6 で紹介します。\n\n\n\n\nBirdNETの利用方法",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>モデルの訓練（GUI）</span>"
    ]
  },
  {
    "objectID": "train_gui.html#sec-gui-win-install",
    "href": "train_gui.html#sec-gui-win-install",
    "title": "4  モデルの訓練（GUI）",
    "section": "4.2 BitdNETのインストール（Windows）",
    "text": "4.2 BitdNETのインストール（Windows）\nWindowsにおけるBirdNETのインストール方法はYouTubeでの解説がわかりやすいです。英語ですが、自動翻訳の字幕で日本語を選択することができます。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>モデルの訓練（GUI）</span>"
    ]
  },
  {
    "objectID": "train_gui.html#sec-gui-train",
    "href": "train_gui.html#sec-gui-train",
    "title": "4  モデルの訓練（GUI）",
    "section": "4.3 訓練",
    "text": "4.3 訓練\nBirdNETのインストールを終えてソフトを立ち上げたら（ソフトの立ち上がりに少し時間がかかると思います）、datasetフォルダに含まれている音声を利用してモデルの訓練を行ってみます。\nタブからTrainを選択し、Select training dataから、先ほど用意したdatasetのフォルダを選択してください。少し時間をおいて、訓練データに含まれていた、BUFJAP、ZHASCH、backgroundの３つのクラスが表示されるはずです (Figure 4.1)。\n\n\n\n\n\n\nFigure 4.1: BirdNET-Analyzerの訓練画面\n\n\n\n続いて、訓練したモデルの保存先を Select classifier output から指定します。ここではdatasetと同じfrog_DL_exampleフォルダのなかに、modelというフォルダを新規に作って選択します。\nその下にいろいろな訓練時のオプションがありますが、デフォルト設定のまま下にスクロールし、Start training のボタンを押して訓練を開始します。\n訓練データセットの読み込みから始まり、訓練過程を示す情報が現れたのちに、次のようなグラフが出力されたら完了です（Figure 4.2）。\n\n\n\n\n\n\nFigure 4.2: 学習曲線\n\n\n\nこのグラフはモデルの学習曲線であり、訓練が進む（Epochの値が高くなる）ほど、モデルの評価指標であるAUPRCやAUROCが向上する様子が見られます。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>モデルの訓練（GUI）</span>"
    ]
  },
  {
    "objectID": "train_gui.html#sec-gui-options",
    "href": "train_gui.html#sec-gui-options",
    "title": "4  モデルの訓練（GUI）",
    "section": "4.4 訓練オプション",
    "text": "4.4 訓練オプション\n続いて autotune 機能を利用してモデルの訓練を行ってみます。先ほどは様々なパラメータを変更せずデフォルトのまま訓練しましたが、訓練時の学習率やモデルの構造などを決めるパラメータ（ハイパーパラメータ）を様々に操作して、最適な値の探索を行うオプションが Use autotune と書かれたチェック項目です。ここにチェックを入れると、ベイズ最適化によるハイパーパラメータ探索が行われます。\nハイパーパラメータの値の組み合わせをどれだけ探索するかはTrialsの数で変更できます。ここではTrialsをデフォルトの50のままにして訓練を行ってみます。Select classifier outputの欄で、モデルを保存する名称をデフォルト（CustomClassifier）から変更し、たとえばCustomClassifier_autotuneなどとして訓練開始（Start training）します。結果を Figure 4.3 に示します。\n\n\n\n\n\n\nFigure 4.3: autotune機能を使った場合の学習曲線\n\n\n\n先ほどのデフォルトのモデルですでに高い性能が出ていたため、パラメータの調整を行っても最終的なAUPRC,AUROCの値にはわずかな違いしか見られませんでした。\n\n\n\n\n\n\nNote\n\n\n\nモデルの訓練過程にはランダム性があるため、実際に訓練してみても、同じ学習曲線にはならないかと思います。\n\n\n最適と判断されたハイパーパラメータは、modelフォルダに出力された、CustomeClassifier_autotune_Params.csvから確認できます。\n\n\n\n\nGhani, Burooj, Tom Denton, Stefan Kahl, and Holger Klinck. 2023. “Global Birdsong Embeddings Enable Superior Transfer Learning for Bioacoustic Classification.” Sci. Rep. 13 (1): 22876. https://doi.org/10.1038/s41598-023-49989-z.\n\n\nKahl, Stefan, Connor M Wood, Maximilian Eibl, and Holger Klinck. 2021. “BirdNET: A Deep Learning Solution for Avian Diversity Monitoring.” Ecol. Inform. 61: 101236. https://doi.org/10.1016/j.ecoinf.2021.101236.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>モデルの訓練（GUI）</span>"
    ]
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "5  データの解析",
    "section": "",
    "text": "5.1 はじめに\nモデルの訓練（GUI）(Chapter 4)によって、ニホンヒキガエルとシュレーゲルアオガエルの鳴き声を判定できるカスタムモデルが作成できました。次はこのモデルを使って訓練データに利用していないデータを解析してみます。\nそのためのデータはsample_data/testのフォルダに入っていることを確認してください。ニホンヒキガエルが鳴いている音声が一つ、またシュレーゲルアオガエルが鳴いている音声が一つです。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>データの解析</span>"
    ]
  },
  {
    "objectID": "inference.html#sec-inference-run",
    "href": "inference.html#sec-inference-run",
    "title": "5  データの解析",
    "section": "5.2 実行",
    "text": "5.2 実行\n訓練時と同様に、BirdNETのGUIを起動してください。\nBatch analysisのタブから、Select input directory (recursive)をクリックし、sample_data/testを選択すると、解析予定のファイル一覧が表示されます（Figure 5.1）。\n\n\n\n\n\n\nFigure 5.1: 解析の設定\n\n\n\n続いて、画面の下の方に行って Species selection &gt; Custom Classifier にチェック。Select classifierから、先程訓練したモデルを選択します。Output settingsとしてはデフォルトのRaven selection tableのほかにも好みの形式を選ぶことができます（Figure 5.2）。\n\n\n\n\n\n\nFigure 5.2: 解析の設定つづき\n\n\n\nこの設定でAnalyzeすると、sample_data/testフォルダの中に、「~BirdNET.selection.table.txt」が末尾についたファイルが生成されます。\n結果をRavenで確認してみると、Figure 5.3 のようになりました（Ravenを立ち上げ、音声ファイル、BirdNETの出力ファイルの順にドラッグ・アンド・ドロップすることで結果を確認可能です）。\n\n\n\n\n\n\n\n\nニホンヒキガエルの解析結果\n\n\n\n\n\n\n\n\n\nシュレーゲルアオガエルの解析結果\n\n\n\n\n\n\nFigure 5.3: 解析結果\n\n\n\n青色で囲われた部分が、BirdNETモデルが検出した各種の鳴き声です。BirdNETでは３秒間が解析単位なので、鳴き声の開始点と終了点が正確に出力されるわけではありませんが、どちらの種も、鳴き声を含んだ音声セグメントをよく検出できています。\nどれだけよく検出できているかを、より具体的に評価する方法は、Chapter 7 で扱います。\n\n\n\n\n\n\nNote\n\n\n\nこのページを用意する過程で、最初は現在の３分の１ほどのデータ量（音声ファイルとしては10ファイル、3秒間に区切った後の数では各種約45サンプルずつ）でモデルを訓練していました。その結果、モデルはシュレーゲルアオガエルはよく検出できたのですが、ヒキガエルは見落としが多く、検出できていた場合もConfidence Score（モデルの確信度）が低いものでした。そこでデータを追加して訓練し直したところ、性能が向上し、ヒキガエルもよく検出できるようになりました。\n今回サンプルデータとして用いたのは、山中の小さな池で、背景雑音（川の流れや車の音など）がほとんどせず、カエルもレコーダと距離が近い狭い範囲で鳴くため、かなり識別しやすい音声になっています。音響モニタリングではもっと難しい条件の地点も珍しくなく、これほど容易には学習が進まないケースもあります。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>データの解析</span>"
    ]
  },
  {
    "objectID": "use_cli.html",
    "href": "use_cli.html",
    "title": "6  CLIによる利用",
    "section": "",
    "text": "6.1 はじめに\n論文等のため解析手順を正確に記録し、報告する場合には、CLIを使うと便利です。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>CLIによる利用</span>"
    ]
  },
  {
    "objectID": "use_cli.html#sec-cli-install",
    "href": "use_cli.html#sec-cli-install",
    "title": "6  CLIによる利用",
    "section": "6.2 インストール",
    "text": "6.2 インストール\nBirdnETをコマンドライン（CLI）で利用するには、Windowsの場合はGUIを使うときにインストールしたもので十分なようです。Macの場合は以下の手順を踏みます。公式ドキュメントの説明が少し簡素で個人的につまづいたため、若干の補足をしています。\n\n6.2.1 gitの導入\ngit（バージョン管理ソフト）を使えるようにする。このためにターミナルを開いて\nxcode-select --install\nと入力し、xcodeをインストールする。\n\n\n6.2.2 BirdNET-Analyzerリポジトリのコピー\n任意のフォルダにGitHubのBirdNETリポジトリをコピーする。\ncd ~/YOUR_FOLDER\ngit clone https://github.com/kahst/BirdNET-Analyzer.git\nここで、YOUR_FOLDERには音声データの解析フォルダに設定したいフォルダまでのパスを入力します。これにより指定したフォルダにBirdNETのソースコードがすべてコピーされます。\n\n\n6.2.3 仮想環境の構築\n適切なバージョンのPython（3.9以上3.11以下）で仮想環境を構築。Pythonのバージョン管理にcondaを用いる場合は、たとえばminicondaをインストールしてから、以下のコマンドをターミナルに入力して仮想環境を構築します。\nconda create -n birdnet python=3.10\nここでbirdnetが新規仮想環境名で、birdnet以外にも好きな名前を入力できます。うまく仮想環境構築されたか確認するため、まず仮想環境をアクティブにします。\nconda activate birdnet\nターミナルの冒頭の文字が(base)から(birdnet)に変更されたら、Pythonのバージョンを確認します。\npython --version\nPython 3.10.XX のように表示されたら成功です。\n\n\n\n\n\n\nWarning\n\n\n\nBirdNETが依存しているTensorflow 2.15.0に適合するPythonのバージョンは3.9から3.11のようです。単純に最新のpythonをインストールすると、3.11よりも新しいものがインストールされて使えませんので、仮想環境の構築では明示的にPythonのバージョンを指定する必要があります。少なくとも私の環境ではPython 3.10であれば問題なく利用できました。\n\n\n\n\n\n\n\n\nNote\n\n\n\n公式ドキュメントではvenvを用いて仮想環境の構築を行っているため、condaを用いたここでのコードと少し異なっています。venvでも仮想環境を管理できますが、単体ではPythonバージョンを自由に指定することができないため、Homebrewと組み合わせて扱うとよいようです。個人的にHomebrewを利用していないのでこれ以上詳しく書けませんが、Symes et al. (2023) によるBirdNET Analyzer GUI MacOS install notesにやり方が書かれています。\n\n\n\n\n6.2.4 Pythonライブラリのインストール\n適切なライブラリをインストールする。仮想環境がアクティブになっている（ターミナルの冒頭の文字が(base)から(birdnet)に変更されている）ことを確認してから、以下のコマンドを入力します。\npip insall tensorflow==2.15.0 tensorflow-metal\nこれはディープラーニングを用いるときによく利用されるライブラリであるTensorfowをインストールしています。またtensorflow-metalはmacOSでTensorflowを利用するためのライブラリです。同様に、他にも必要なものをインストールします。\npip install librosa resampy\n訓練時にautotune機能を用いる場合は、加えて\npip install keras-tuner\nMacでGUIを使いたい場合は、\npip install pywebview gradio\nをさらにインストールします。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>CLIによる利用</span>"
    ]
  },
  {
    "objectID": "use_cli.html#sec-cli-gui",
    "href": "use_cli.html#sec-cli-gui",
    "title": "6  CLIによる利用",
    "section": "6.3 MacでのGUIの利用",
    "text": "6.3 MacでのGUIの利用\nGUIを使うためには、GitHubからコピーしたBirdNET-AnalyzerフォルダをFinder上で右クリックし、「フォルダに新規ターミナル」を選択します。立ち上がったターミナルに以下のコマンドを入力し、GUIを起動させます。\nconda activate birdnet\npython -m gui\nその後はWindowsでのGUIの使い方（Chapter 4）と同じです。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>CLIによる利用</span>"
    ]
  },
  {
    "objectID": "use_cli.html#sec-cli-code",
    "href": "use_cli.html#sec-cli-code",
    "title": "6  CLIによる利用",
    "section": "6.4 CLIによる訓練と解析",
    "text": "6.4 CLIによる訓練と解析\nCLIによりモデルの訓練を行う場合、先ほど（Section 6.3）と同様にBirdNET-Analyzerフォルダにターミナルを立ち上げ、以下のコードを打ち込みます。\nconda activate BirdNET\npython train.py --i PATH/TO/TRAIN/DATASET --o model/CustomClassifier_autotune --autotune \nPATH/TO/TRAIN/DATASETの部分には訓練データへのパスを記入します。\nまた、データ解析も同様に簡単なコードにより実行できます。\npython analyze.py --i PATH/TO/AUDIO/DATA --o output/BirdNET-output --classifier model/CustomClassifier_autotune.tflite\n詳細なオプションは公式のドキュメントを確認してください。\n\n\n\n\nSymes, Laurel, Larissa S M Sugai, Benjamin Gottesman, Michael Pitzrick, and Connor Wood. 2023. “Acoustic analysis with BirdNET and (almost) no coding: practical instructions.” Zenodo. https://doi.org/10.5281/zenodo.8357176.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>CLIによる利用</span>"
    ]
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "7  性能評価",
    "section": "",
    "text": "7.1 性能の評価指標\n機械学習モデルの性能評価指標の一つとして、モデルの訓練（Chapter 4）で登場したAUPRCやAUROCがあります。BirdNETではデフォルトで訓練データの20%をモデルの評価に利用してこれらの値を算出しています。\nその他のよく用いられる評価指標にPrecisionとRecallがあります。それぞれ以下の式で定義されます。\n\\[\n\\begin{aligned}\nPrecision = \\frac{TP}{TP + FP}, \\\\\nRecall = \\frac{TP}{TP + FN}\n\\end{aligned}\n\\]\nここでTP, FP, FNはそれぞれ真陽性（true positives）、偽陽性（false positives）、偽陰性（false negatives）を表します。\nすなわち、precisionは「モデルがこのカエルだと判断したサンプルのうち、実際にそのカエルが鳴いていたサンプルの割合」を、recallは「あるカエルが鳴いているすべてのサンプルのうち、モデルが正しく検出できたサンプルの割合」を表します。どちらも1に近いほどモデルの性能が良いことを表します。Precisionとrecallは一般にトレードオフの関係にあり、モデルの検出閾値を高めることで偽陽性（FP）が減ってprecisionは向上するものの、偽陰性（FN）が増えてrecallの値が低下します。\nPrecisionとrecallは、AUROC等と比べて値の意味が解釈がしやすく、解析結果がどれだけ信用できるものなのか判断するうえで役立つため、以下でそれを算出するコードの一例を紹介します。このページの内容はサンプルデータ内にある、test.qmdを開いて実行することができます。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>性能評価</span>"
    ]
  },
  {
    "objectID": "test.html#sec-test-calc",
    "href": "test.html#sec-test-calc",
    "title": "7  性能評価",
    "section": "7.2 評価指標の算出",
    "text": "7.2 評価指標の算出\n評価指標の算出にはcaretパッケージを用います。訓練済みモデルの予測と、対応するアノテーションデータはsample_data/performanceの中に格納されています。\n\nlibrary(tidyverse)\nlibrary(caret)\n\n\n# データの一覧取得\nannotation_list &lt;- list.files(\"sample_data/performance\", \"*selections.txt\", full.names = TRUE)\nBirdNET_results_list &lt;- list.files(\"sample_data/performance\", \"*BirdNET.selection.table.txt\", full.names = TRUE)\n\nまずニホンヒキガエル（BUFJAP）についての性能評価を行います。BirdNETの出力や、アノテーションデータからラベルを取得するために、create_labels()関数を少し変更したもの以下に定義します。\n\ncreate_labels2 &lt;- function(df){\n  label_vec &lt;- vector(mode = \"character\")  # ラベルを格納するベクトルを初期化\n  \n  for(i in 1:20){\n    start &lt;- (i-1) * 3  \n    end &lt;- i * 3  \n    label_df &lt;- df |&gt; \n      # startからendと重複する部分のあるアノテーションのみを抜き出す\n      filter(`Begin Time (s)` &lt; end & `End Time (s)` &gt; start) |&gt; \n      distinct(Annotation)  # 同じ種のラベルの重複を削除\n    \n    # クリップ内にアノテーションがない場合は\"background\"、ある場合はアノテーションをラベルにする\n    label &lt;- if(nrow(label_df) == 0) \"background\" else label_df$Annotation\n    label_vec[i] &lt;- label  # ラベルをリストに格納\n  }\n  \n  return(label_vec)\n}\n\nこれを用いて、モデルの予測をfactor形式で取得します。\n\npred_list &lt;- annotation_list |&gt; \n  map(function(x) x |&gt; \n        read_delim(delim = \"\\t\") |&gt; \n        filter(Annotation == \"BUFJAP\") |&gt;  # ニホンヒキガエルのみに着目\n        create_labels2()\n        )\n\n# モデルの予測をfactor形式に変換\npred &lt;- pred_list |&gt; unlist() |&gt; as.factor()\n\nここでmap()は、リスト（annotation_list）内のすべての要素に対して、function(x)以下の関数を作用させる機能を持ちます。同様に、訓練したBirdNETモデルの予測をfactor形式で取得します。\n\ntruth_list &lt;- BirdNET_results_list |&gt; \n  map(function(x) x |&gt; \n        read_delim(delim = \"\\t\") |&gt; \n        mutate(Annotation = `Species Code`) |&gt;  # create_labels2()が機能するように列を追加\n        filter(Annotation == \"BUFJAP\") |&gt; \n        create_labels2()\n        )\n\ntruth &lt;- truth_list |&gt; unlist() |&gt; as.factor()\n\n正解ラベル（truth）とモデルの予測（pred）から、confusionMatrix()によりどれだけ正解していたのか確認します。\n\nconfusionMatrix(pred, truth, positive = \"BUFJAP\", mode = \"prec_recall\")\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   background BUFJAP\n  background         21      0\n  BUFJAP              0     19\n                                     \n               Accuracy : 1          \n                 95% CI : (0.9119, 1)\n    No Information Rate : 0.525      \n    P-Value [Acc &gt; NIR] : 6.403e-12  \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n                                     \n              Precision : 1.000      \n                 Recall : 1.000      \n                     F1 : 1.000      \n             Prevalence : 0.475      \n         Detection Rate : 0.475      \n   Detection Prevalence : 0.475      \n      Balanced Accuracy : 1.000      \n                                     \n       'Positive' Class : BUFJAP     \n                                     \n\n\n今回の場合、precision、recallともに100%で、ニホンヒキガエルに関する予測は完璧だったようです。\nこのようなprecisionとrecallの算出を、シュレーゲルアオガエル（ZHASCH）についても行います。\n\nspecies = \"ZHASCH\"\n\n# モデルの予測\npred &lt;- annotation_list |&gt; \n  map(function(x) x |&gt; \n        read_delim(delim = \"\\t\") |&gt; \n        filter(Annotation == species) |&gt; \n        create_labels2()\n        ) |&gt; \n  unlist() |&gt; \n  as.factor()\n\n# 正解ラベル\ntruth &lt;- BirdNET_results_list |&gt; \n  map(function(x) x |&gt; \n        read_delim(delim = \"\\t\") |&gt; \n        mutate(Annotation = `Species Code`) |&gt;  # create_labels2()が機能するように列を追加\n        filter(Annotation == species) |&gt; \n        create_labels2()\n        ) |&gt; \n  unlist() |&gt; \n  as.factor()\n\n\n# 混同行列と評価指標の算出\nconfusionMatrix(pred, truth, positive = species, mode = \"prec_recall\")\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   background ZHASCH\n  background         36      0\n  ZHASCH              0      4\n                                     \n               Accuracy : 1          \n                 95% CI : (0.9119, 1)\n    No Information Rate : 0.9        \n    P-Value [Acc &gt; NIR] : 0.01478    \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n                                     \n              Precision : 1.0        \n                 Recall : 1.0        \n                     F1 : 1.0        \n             Prevalence : 0.1        \n         Detection Rate : 0.1        \n   Detection Prevalence : 0.1        \n      Balanced Accuracy : 1.0        \n                                     \n       'Positive' Class : ZHASCH     \n                                     \n\n\n今回のケースでは、モデルの性能は高く、1分間×2つの音声ファイルについて、ニホンヒキガエルとシュレーゲルアオガエルのprecison、recallはいずれも100％という結果になりました。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>性能評価</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "引用文献",
    "section": "",
    "text": "Balvanera, Santiago Martinez, Oisin Mac Aodha, Matthew J Weldy, Holly\nPringle, Ella Browning, and Kate E Jones. 2023. “Whombat: An\nOpen-Source Annotation Tool for Machine Learning Development in\nBioacoustics.” arXiv. https://arxiv.org/abs/2308.12688.\n\n\nCañas, Juan Sebastián, María Paula Toro-Gómez, Larissa Sayuri Moreira\nSugai, Hernán Darío Benítez Restrepo, Jorge Rudas, Breyner Posso\nBautista, Luís Felipe Toledo, et al. 2023. “A Dataset for\nBenchmarking Neotropical Anuran Calls Identification in Passive Acoustic\nMonitoring.” Sci Data 10 (1): 771. https://doi.org/10.1038/s41597-023-02666-2.\n\n\nChronister, Lauren M, Tessa A Rhinehart, Aidan Place, and Justin Kitzes.\n2021. “An Annotated Set of Audio Recordings of Eastern North\nAmerican Birds Containing Frequency, Time, and Species\nInformation.” Ecology 102 (6): e03329. https://doi.org/10.1002/ecy.3329.\n\n\nGhani, Burooj, Tom Denton, Stefan Kahl, and Holger Klinck. 2023.\n“Global Birdsong Embeddings Enable Superior Transfer Learning for\nBioacoustic Classification.” Sci. Rep. 13 (1): 22876. https://doi.org/10.1038/s41598-023-49989-z.\n\n\nKahl, Stefan, Connor M Wood, Maximilian Eibl, and Holger Klinck. 2021.\n“BirdNET: A Deep Learning Solution for Avian\nDiversity Monitoring.” Ecol. Inform. 61: 101236. https://doi.org/10.1016/j.ecoinf.2021.101236.\n\n\nKimura, Kaede, and Teiji Sota. 2023. “Evaluation of Deep\nLearning-Based Monitoring of Frog Reproductive Phenology.”\nIchthyology & Herpetology 111 (4): 563–70. https://doi.org/10.1643/h2023018.\n\n\nSteinfath, Elsa, Adrian Palacios-Muñoz, Julian R Rottschäfer, Deniz\nYuezak, and Jan Clemens. 2021. “Fast and Accurate Annotation of\nAcoustic Signals with Deep Neural Networks.” eLife 10:\ne68837. https://doi.org/10.7554/eLife.68837.\n\n\nSymes, Laurel, Larissa S M Sugai, Benjamin Gottesman, Michael Pitzrick,\nand Connor Wood. 2023. “Acoustic analysis\nwith BirdNET and (almost) no coding: practical\ninstructions.” Zenodo. https://doi.org/10.5281/zenodo.8357176.\n\n\n木村楓. 2024.\n“ディープラーニングによるカエル類の音響モニタリング入門.”\n爬虫両棲類学会報 2024 (2).",
    "crumbs": [
      "引用文献"
    ]
  }
]