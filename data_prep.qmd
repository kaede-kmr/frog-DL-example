# 訓練データの整理 {#sec-prep}

## はじめに {#sec-prep-intro}

続いて、アノテーションされた音声を訓練データとして扱いやすい形に変換します。

今回扱うBirdNETモデルの場合、３秒間の音声データを１サンプルとして訓練や解析を行う仕様です。３秒以上の音声ファイルは中央の３秒間が抽出され、３秒未満なら３秒間になるようノイズが付加されます（[公式ドキュメント](https://github.com/kahst/BirdNET-Analyzer?tab=readme-ov-file#8-training)）。ディープラーニングのモデルでは、多くの場合、計算を高速化するため特定の時間間隔で音声データを区切って扱う仕様になっています。

BirdNETが必要とする訓練データを用意するため、アノテーションした音声データに以下の処理を行います。

1. 音声データを３秒ずつに区切る

1. カエルの種ごとにフォルダを作成し、その中に３秒間の音声データを入れていく

  - たとえば３秒間の音声にヒキガエルが含まれているならBUFJAPという名称のフォルダに入れ、シュレーゲルアオガエルが含まれているならZHASCHに入れます。
  - どの種も含まれていない背景音ならbackgroundという名称のフォルダを作って、そこに入れます。
  - ヒキガエルとシュレーゲルアオガエルの両種が含まれる場合は、保存フォルダの名称を「BUFJAP,ZHASCH」のようにカンマ区切りで両種の名前をつけます（種名の間にはスペースを入れない）。

以下、Rを用いてその作業を自動化するサンプルコードを提示します。このページの内容はサンプルデータ内にある、data_prep.qmdを開いて実行することができます。


## Rの利用 {#sec-prep-useR}

以下の内容はRの知識を必要とします。Rは統計解析に重きをおいたプログラミング言語です。Rについて入門書が多数出ており、そちらを参照するのも良いですし、オンライン上でも[私たちのR](https://www.jaysong.net/RBook/)や、[R for Data Science (2e)](https://r4ds.hadley.nz/)の丁寧な解説を読むことができます（後者の日本語版は「Rではじめるデータサイエンス 第2版」として出版）。

生態学分野ではRを利用している人が多いためRでのコードを記しますが、Pythonなど好みの言語で同様の処理を行うことができます。

::: callout-note
今回紹介しているディープラーニングによる音響解析手順のなかで、このデータ整理の部分はあまりシンプルにできておらず、それなりに長いRのコードを解釈する必要があります。RavenとBirdNETがより統合されてくると、このあたりももっと簡単になるかもしれません。
:::

## セットアップ {#sec-prep-setup}

以下のパッケージを使用します。

```{r message=FALSE}
# 必要なライブラリの読み込み
library(tuneR)
library(tidyverse)
```

::: callout-note
以下に記載しているコードが適切に実行されるには、Rのワーキングディレクトリの直下にsample_dataフォルダがある必要があります。ダウンロードしたサンプルデータ・コード（@sec-index-dataset）から、frog-DL-example.RprojをクリックしてRを開くと、ワーキングディレクトリが適切に設定されるはずです。

なおワーキングディレクトリはコンソールに`getwd()`と打ち込んで確認できます。
:::

まずサンプルデータに含まれているアノテーションを確認します。このために、`list.files()`関数によりsample_data/trainフォルダに入っているアノテーションファイルの一覧を取得し、その冒頭6個を表示させてみます。

```{r}
annotations <- list.files("sample_data/train", "selections.txt")
print(head(annotations))
```

各ファイルは{録音日}_{録音時刻}.Table.1.selections.txt という命名規則になっています。
これらのアノテーションファイルに対応する音声もフォルダ内にあり、そのファイル名は録音日時を表す部分に拡張子.WAV がついた形をしています。

```{r}
audio <- list.files("sample_data/train", "WAV")
print(head(audio))
```

最終的にこれらの音声ファイルを３秒間ずつに区切り、鳴いている種の名前がついたフォルダに保存していくのですが、そのために使ういくつかの関数を定義していきます。

## 関数の定義

### ラベル作成関数

Ravenで作成されたselections.txtファイルを読み込むには`read_delim()`関数を用います。このファイルは表の各列がタブによって区切られた形式をしているため、delim = \\t と指定することで、適切に読み込むことができます。たとえば

```{r}
df <- read_delim(file.path("sample_data/train", annotations[1]), delim = "\t", show_col_types = FALSE)
knitr::kable(df)
```

なお末尾の show_col_types は、列の形式（数値か文字列かなど）を表示するかどうかを決めるオプションです。また、`knitr::kable()`部分は、オンライン上で表が綺麗に表示されるためだけのもので、ここでは無視して問題ありません。

つづいて３秒間ずつ（０から３秒目、３から６秒目、６から９秒目...のように）アノテーションされた種を抜き出すことを考えます。例えば前述の表で、15から18秒目までに含まれるアノテーションを抜き出すには、

```{r}
label_df <- df |> 
  filter(`Begin Time (s)` < 18 & `End Time (s)` > 15) |> 
  distinct(Annotation)  # 同じ種のラベルがあった場合に重複を削除する

knitr::kable(label_df)
```

15から18秒の間ではニホンヒキガエルの鳴き声が一つ含まれていましたが、何の種も鳴いていない場合、あるいは複数種が鳴いている場合もあります。これに対処するため、以下のようにラベルを取得します。

```{r}
label_df <- df |> 
  filter(`Begin Time (s)` < 3 & `End Time (s)` > 0) |>   # ０から３秒目には何も鳴いていない
  distinct(Annotation) |> 
  arrange(Annotation)  # アノテーションをアルファベット順に並べ替え

label <- if(nrow(label_df) == 0) "background" else str_flatten(label_df$Annotation, collapse = ",")
print(label)
```

ここでアノテーションが含まれない部分は、表の行数が0になることを利用し、`if(nrow(label_df) == 0)`で判定して、backgroundというラベル名をつけています。また複数種の鳴き声が含まれる場合は、`str_flatten()`関数によりカンマで連結されたラベル名とします（例：BUFJAP,ZHASCH）。

ここまでのことを利用して、アノテーションファイルを読み込み、3秒ごとのラベルを作成する関数を作成します。０から３秒目、３から６秒目...と逐次的な操作を行うため、`for`文を用いて繰り返しの処理を行っています。

```{r}
create_labels <- function(annotation){
  df <- read_delim(annotation, delim = "\t", show_col_types = FALSE) |>   # アノテーションファイルを読み込む
    filter(Annotation != "frog")  # 種不明を表すfrogを除く
  
  label_vec <- vector(mode = "character")  # ラベルを格納するベクトルを初期化
  
  for(i in 1:20){
    start <- (i-1) * 3  # i番目の音声クリップの開始時間
    end <- i * 3  # i番目の音声クリップの終了時間
    # 音声クリップ内に含まれるアノテーションを整理
    label_df <- df %>%
      # startからendと重複する部分のあるアノテーションのみを抜き出す
      filter(`Begin Time (s)` < end & `End Time (s)` > start) %>%  
      distinct(Annotation) %>%  # 同じ種のラベルの重複を削除
      arrange(Annotation)  # アノテーションをアルファベット順に並べ替え
    
    # クリップ内にアノテーションがない場合は"background"、ある場合はアノテーションをカンマ区切りで連結
    label <- if(nrow(label_df) == 0) "background" else str_flatten(label_df$Annotation, collapse = ",")
    label_vec[i] <- label  # ラベルをリストに格納
  }
  
  return(label_vec)
}
```

この関数をテストしてみます。

```{r}
labels <- create_labels(file.path("sample_data/train", annotations[1]))
print(labels)
```

60秒間を3秒毎に区切ったので、20個のラベルが作成されました。ニホンヒキガエルの鳴き声が含まれる音声クリップについてはBUFJAP、アノテーションがついていない音声クリップについてはbackgroundというラベルがついています。


### 音声の分割

続いて、音声ファイルを読み込み、３秒ごとの短い音声クリップに分割する関数を定義します。まずRで音声ファイルを扱うためにtuneRの`readWave()`関数により音声データを読み込みます。

```{r}
wave <- readWave(file.path("sample_data/train", audio[1]))
```

この音声（audio[1] = `r audio[1]` ）の概要を確認すると、

```{r}
print(wave)
```

60秒間（Durationに着目）のモノラル音源で、サンプリング周波数（Samplingrate）は32000 Hz であることがわかります。この60秒の音声のうち、０から３秒目までは以下のように取得できます。

```{r}
y <- wave@left          # 音声データの具体的な数値を取得
sr <- wave@samp.rate    # サンプリング周波数を取得

clip <- y[1 : (3*sr)]   # ０から３秒目までを取得
```

ここで、音声データは毎秒サンプリング周波数個（この音源では32000個）の数値によって表現されているため、１から「3×サンプリング周波数」番目までの音声データを切り取ると冒頭３秒の音声を取得できることに注意してください。

以上をふまえて、音声を3秒ずつにカットする関数を定義します。

```{r}
cut_audio <- function(audio_file){
  wave <- readWave(audio_file)    # 音声ファイルの読み込み
  y <- wave@left                  # 波形データを取得
  sr <- wave@samp.rate            # サンプリング周波数を取得
  
  # 分割された音声を格納するリストを作成
  audio_list <- list()
  
  for(i in 1:20){
    start <- (i-1) * 3 * sr + 1   # 音声クリップの開始位置を計算
    end <- i * 3 * sr             # 音声クリップの終了位置を計算
    clip <- y[start:end]          # 3秒ごとに分割
    audio_list[[i]] <- Wave(left = clip, samp.rate = sr, bit = 16)  # 分割した音声をリストに格納
  }
    
  return(audio_list)
}

```

これも関数がどんな働きをするか、テストしてみます。

```{r}
test <- cut_audio(file.path("sample_data/train", audio[1]))
print(test[1:3])
```

このように、３秒ずつに区切られた音声データ（Durationに着目）が生成されています。


### 音声とラベルの保存

音声データの保存には`writeWave()`関数が利用できます。この関数は、第一引数として保存する音声データを、第二引数として保存する際のファイル名をとります。音声データは先程定義した`cut_audio()`関数により取得できますので、保存先のフォルダと、保存する際のファイル名を作成することを考えます。

まず保存先のフォルダとしてdatasetという名称のフォルダを作り、その中にbackground,BUFJAP,ZHASCHといった種名のフォルダを含めることにします。

```{r}
save_dir <- file.path("dataset", labels[1])
save_dir
```

すでにdataset/backgroundのようなフォルダが存在している場合にはこれで問題なく音声を保存できますが、最初の一回はフォルダの作成から始める必要があります。Rでは``dir.create()`によりフォルダを作成することが出来ます。

```{r}
dir.create(save_dir, recursive = TRUE, showWarnings = FALSE)  # フォルダが存在しない場合は作成
```

これにより、ワーキングディレクトリにdataset/backgroundという空のフォルダが作成されます。ラベルがbackgroundではなくBUFJAPやZHASCHであった場合にも、それぞれの種名がついたフォルダが自動で作成されます。

また、保存する際のファイル名（上流のパスを含む）は、以下のように設定できます。ここでは{録音日}\_{録音時刻}\_{番号}.WAVという命名規則でファイル名をつけています。

```{r}
file.path(
  "dataset",
  labels[1],
  str_replace(audio[1], ".WAV", str_c("_", "1", ".WAV"))
)
```


## 処理の実行

ここまで定義してきた関数を用いて、全てのアノテーションファイルに対して上記の処理を実行します。

```{r eval=FALSE}
# アノテーションファイル一覧を取得
annotations <- list.files("sample_data/train", pattern = "selections.txt")

# 音声クリップへの分割と保存を実行
for(file in annotations){
  labels <- create_labels(file.path("sample_data/train", file))          # ラベルを作成
  audio_file <- str_replace(file, "Table.1.selections.txt", "WAV")       # アノテーションに対応する音声ファイル名を取得
  cutted_audio <- cut_audio(file.path("sample_data/train", audio_file))  # 音声ファイルを分割
  
  for(i in 1:20){
    # フォルダが存在しない場合は作成
    dir.create(file.path("dataset", labels[i]), recursive = TRUE, showWarnings = FALSE)         
    # 音声を保存
    writeWave(object = cutted_audio[[i]],
              filename = file.path("dataset",
                                   labels[i], 
                                   str_replace(audio_file, ".WAV", str_c("_", i, ".WAV"))),  # ファイル名に番号(1,2,3...)をつけて保存
              extensible = FALSE  # モノラルでも片耳からの音声にならないよう設定
              )
  }
}

```

うまくいっていれば、現在の作業ディレクトリにdatasetというフォルダが作成され、そのなかに「background」「BUFJAP」「BUFJAP,ZHASCH」「ZHASCH」があり、それぞれに対応する音声ファイルが保存されているという状況ができたはずです。

次は、**モデルの訓練**に移ります。
